# Text Classification Pipeline with Hugging Face Transformers

A complete text classification pipeline using Hugging Face Transformers, implementing preprocessing, model fine-tuning, evaluation, and deployment utilities. This project demonstrates best practices for NLP and showcases how to build an end-to-end pipeline for text classification tasks.

## Features

- **Comprehensive Pipeline**: Complete workflow from data preprocessing to model deployment
- **Modular Design**: Well-structured code organization with separated components
- **Multiple Datasets**: Support for IMDB, AG News, Yelp Reviews, and custom datasets
- **Multilingual Support**: Cross-lingual transfer learning and multilingual model fine-tuning
- **Advanced Evaluation**: F1, precision, recall metrics with visualizations and error analysis
- **Hyperparameter Tuning**: Automated tuning with Optuna
- **Model Comparison**: Tools to compare multiple model architectures
- **Deployment Ready**: Inference API code for model deployment

## Project Structure

```
/
â”œâ”€â”€ notebooks/                         # Jupyter notebooks
â”‚   â”œâ”€â”€ data_exploration.ipynb         # Dataset analysis
â”‚   â”œâ”€â”€ model_training.ipynb           # Interactive training
â”‚   â””â”€â”€ evaluation_analysis.ipynb      # Results visualization
â”‚
â”œâ”€â”€ src/                               # Source code
â”‚   â”œâ”€â”€ train_model.py                 # Main training script
â”‚   â”œâ”€â”€ data_preprocessing.py          # Text preprocessing utilities
â”‚   â”œâ”€â”€ model_utils.py                 # Model handling utilities
â”‚   â””â”€â”€ config.py                      # Configuration management
â”‚
â”œâ”€â”€ models/                            # Model storage
â”‚   â”œâ”€â”€ trained_model/                 # Trained model weights
â”‚   â””â”€â”€ tokenizer/                     # Saved tokenizer files
â”‚
â”œâ”€â”€ reports/                           # Analysis reports
â”‚   â”œâ”€â”€ model_report.md                # Model architecture report
â”‚   â”œâ”€â”€ evaluation_metrics.json        # Detailed metrics
â”‚   â””â”€â”€ confusion_matrix.png           # Visualization outputs
â”‚
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ README.md                          # Project documentation
â”œâ”€â”€ submission.md                      # Approach and key learnings
â”œâ”€â”€ train.py                           # Training script entry point
â””â”€â”€ .gitignore                         # Git ignore configuration
```

## Getting Started

### Prerequisites

- Python 3.8+
- PyTorch 1.12+
- CUDA-compatible GPU (recommended)

### Installation

1. Clone the repository

   ```bash
   git clone
   cd text-classification-pipeline
   ```

2. Set up a virtual environment

   ```bash
   python -m venv venv
   source venv/bin/activate  # Windows: venv\Scripts\activate
   ```

3. Install dependencies
   ```bash
   pip install -r requirements.txt
   ```

### Running the Pipeline

#### Data Exploration

Explore and analyze the dataset:

```bash
jupyter notebook notebooks/data_exploration.ipynb
```

#### Training a Model

Train a model using the default configuration (IMDB dataset with DistilBERT):

```bash
python train.py
```

With custom configuration:

```bash
python train.py --config imdb  # Predefined configs: imdb, ag_news, yelp, multilingual
```

With hyperparameter tuning:

```bash
python train.py --tune_hyperparams
```

#### Evaluating a Model

Run evaluation on a trained model:

```bash
python train.py --eval_only
```

Visualize and analyze results:

```bash
jupyter notebook notebooks/evaluation_analysis.ipynb
```

## Supported Datasets

The pipeline supports the following datasets out of the box:

- **IMDB Movie Reviews**: Binary sentiment classification (positive/negative)
- **AG News**: Topic classification with 4 classes
- **Yelp Reviews**: Binary sentiment classification
- **Custom Datasets**: Support for custom datasets in CSV, JSON, or TSV formats

## ðŸ”§ Multilingual Extension

The pipeline includes multilingual support with:

- Cross-lingual transfer learning
- Language detection and preprocessing
- Multilingual model training with XLM-RoBERTa or mBERT
- Zero-shot evaluation on new languages

## Advanced Evaluation

The evaluation framework includes:

- Precision, recall, and F1 metrics
- Confusion matrix visualization
- ROC curve analysis for binary classification
- Detailed error analysis
- Per-class performance metrics
- Failure case studies

## Model Comparison

Compare multiple models:

```bash
python -m src.train_model --config imdb
python -m src.train_model --config imdb --model_name bert-base-uncased
jupyter notebook notebooks/evaluation_analysis.ipynb  # For comparison visualization
```

## Deployment

The project includes a deployment-ready inference pipeline:

- FastAPI-based REST API
- Streamlit interactive web application
- Batch prediction capabilities
- Production-ready model serving

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- Hugging Face for the Transformers library
- The creators of the datasets used in this project
- The open-source community for various tools and libraries
