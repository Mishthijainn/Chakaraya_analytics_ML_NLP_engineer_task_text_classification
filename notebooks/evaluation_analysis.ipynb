{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training and Experimentation\n",
        "\n",
        "This notebook demonstrates interactive model training and hyperparameter experimentation for the text classification pipeline.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” Evaluation Analysis Environment Ready!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import json\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_recall_fscore_support, \n",
        "    confusion_matrix, classification_report,\n",
        "    roc_curve, auc, roc_auc_score\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    pipeline, Trainer\n",
        ")\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Visualization\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"ðŸ” Evaluation Analysis Environment Ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from models\\trained_model...\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "models\\tokenizer does not appear to have a file named config.json. Checkout 'https://huggingface.co/models\\tokenizer/tree/None' for available files.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 252\u001b[39m\n\u001b[32m    249\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m report_data\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# Initialize evaluator\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m evaluator = \u001b[43mComprehensiveEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./models/trained_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./models/tokenizer\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mComprehensiveEvaluator.__init__\u001b[39m\u001b[34m(self, model_path, tokenizer_path)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mself\u001b[39m.classifier = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mComprehensiveEvaluator.load_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load trained model and tokenizer\"\"\"\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.model = AutoModelForSequenceClassification.from_pretrained(\u001b[38;5;28mself\u001b[39m.model_path)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.classifier = pipeline(\n\u001b[32m     19\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtext-classification\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     20\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m     21\u001b[39m     tokenizer=\u001b[38;5;28mself\u001b[39m.tokenizer,\n\u001b[32m     22\u001b[39m     device=\u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m1\u001b[39m\n\u001b[32m     23\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Mishthi\\chakaralya_analytics_ML_NLP\\venv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:837\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    835\u001b[39m         config = AutoConfig.for_model(**config_dict)\n\u001b[32m    836\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m         config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    840\u001b[39m config_tokenizer_class = config.tokenizer_class\n\u001b[32m    841\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoTokenizer\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config.auto_map:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Mishthi\\chakaralya_analytics_ML_NLP\\venv\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:934\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    931\u001b[39m trust_remote_code = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtrust_remote_code\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    932\u001b[39m code_revision = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m934\u001b[39m config_dict, unused_kwargs = \u001b[43mPretrainedConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    935\u001b[39m has_remote_code = \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    936\u001b[39m has_local_code = \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Mishthi\\chakaralya_analytics_ML_NLP\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:632\u001b[39m, in \u001b[36mPretrainedConfig.get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    630\u001b[39m original_kwargs = copy.deepcopy(kwargs)\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m config_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[32m    634\u001b[39m     original_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = config_dict[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Mishthi\\chakaralya_analytics_ML_NLP\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:689\u001b[39m, in \u001b[36mPretrainedConfig._get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    685\u001b[39m configuration_file = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33m_configuration_file\u001b[39m\u001b[33m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    688\u001b[39m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m689\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    703\u001b[39m     commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[32m    705\u001b[39m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Mishthi\\chakaralya_analytics_ML_NLP\\venv\\Lib\\site-packages\\transformers\\utils\\hub.py:370\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(resolved_file):\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    371\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Checkout \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    372\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m for available files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    373\u001b[39m         )\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    375\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[31mOSError\u001b[39m: models\\tokenizer does not appear to have a file named config.json. Checkout 'https://huggingface.co/models\\tokenizer/tree/None' for available files."
          ]
        }
      ],
      "source": [
        "\n",
        "class ComprehensiveEvaluator:\n",
        "    \"\"\"Advanced evaluation system for text classification models\"\"\"\n",
        "    \n",
        "    def __init__(self, model_path, tokenizer_path):\n",
        "        self.model_path = Path(model_path)\n",
        "        self.tokenizer_path = Path(tokenizer_path)\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.classifier = None\n",
        "        self.load_model()\n",
        "    \n",
        "    def load_model(self):\n",
        "        \"\"\"Load trained model and tokenizer\"\"\"\n",
        "        print(f\"Loading model from {self.model_path}...\")\n",
        "        \n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_path)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
        "        self.classifier = pipeline(\n",
        "            'text-classification',\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "        \n",
        "        print(\"Model loaded successfully!\")\n",
        "    \n",
        "    def evaluate_on_dataset(self, dataset, dataset_name=\"test\"):\n",
        "        \"\"\"Comprehensive evaluation on dataset\"\"\"\n",
        "        print(f\"Evaluating on {dataset_name} dataset...\")\n",
        "        \n",
        "        # Get predictions\n",
        "        texts = dataset['text']\n",
        "        true_labels = dataset['label']\n",
        "        \n",
        "        # Batch predictions for efficiency\n",
        "        batch_size = 32\n",
        "        predictions = []\n",
        "        prediction_scores = []\n",
        "        \n",
        "        for i in tqdm(range(0, len(texts), batch_size)):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "            batch_preds = self.classifier(batch_texts)\n",
        "            \n",
        "            for pred in batch_preds:\n",
        "                predictions.append(pred['label'])\n",
        "                prediction_scores.append(pred['score'])\n",
        "        \n",
        "        # Convert string labels to numeric if needed\n",
        "        label_map = {'NEGATIVE': 0, 'POSITIVE': 1} if 'NEGATIVE' in predictions else None\n",
        "        if label_map:\n",
        "            pred_labels = [label_map[p] for p in predictions]\n",
        "        else:\n",
        "            pred_labels = predictions\n",
        "        \n",
        "        # Calculate metrics\n",
        "        metrics = self.calculate_comprehensive_metrics(true_labels, pred_labels, prediction_scores)\n",
        "        \n",
        "        # Store results\n",
        "        self.evaluation_results = {\n",
        "            'dataset_name': dataset_name,\n",
        "            'predictions': pred_labels,\n",
        "            'true_labels': true_labels,\n",
        "            'prediction_scores': prediction_scores,\n",
        "            'metrics': metrics,\n",
        "            'texts': texts\n",
        "        }\n",
        "        \n",
        "        return self.evaluation_results\n",
        "    \n",
        "    def calculate_comprehensive_metrics(self, y_true, y_pred, scores):\n",
        "        \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision, recall, f1, support = precision_recall_fscore_support(\n",
        "            y_true, y_pred, average='weighted'\n",
        "        )\n",
        "        \n",
        "        # Per-class metrics\n",
        "        per_class_metrics = precision_recall_fscore_support(\n",
        "            y_true, y_pred, average=None\n",
        "        )\n",
        "        \n",
        "        # ROC AUC for binary classification\n",
        "        roc_auc = None\n",
        "        if len(set(y_true)) == 2:\n",
        "            roc_auc = roc_auc_score(y_true, scores)\n",
        "        \n",
        "        metrics = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'roc_auc': roc_auc,\n",
        "            'per_class_precision': per_class_metrics[0],\n",
        "            'per_class_recall': per_class_metrics[1],\n",
        "            'per_class_f1': per_class_metrics[2],\n",
        "            'support': per_class_metrics[3]\n",
        "        }\n",
        "        \n",
        "        return metrics\n",
        "    \n",
        "    def create_confusion_matrix_plot(self, class_names=None):\n",
        "        \"\"\"Create advanced confusion matrix visualization\"\"\"\n",
        "        if not hasattr(self, 'evaluation_results'):\n",
        "            raise ValueError(\"Run evaluation first!\")\n",
        "        \n",
        "        y_true = self.evaluation_results['true_labels']\n",
        "        y_pred = self.evaluation_results['predictions']\n",
        "        \n",
        "        # Calculate confusion matrix\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        \n",
        "        # Normalize confusion matrix\n",
        "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        \n",
        "        # Create subplots\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        \n",
        "        # Raw confusion matrix\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                   xticklabels=class_names, yticklabels=class_names, \n",
        "                   ax=axes[0])\n",
        "        axes[0].set_title('Confusion Matrix (Raw Counts)')\n",
        "        axes[0].set_xlabel('Predicted')\n",
        "        axes[0].set_ylabel('Actual')\n",
        "        \n",
        "        # Normalized confusion matrix\n",
        "        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "                   xticklabels=class_names, yticklabels=class_names,\n",
        "                   ax=axes[1])\n",
        "        axes[1].set_title('Confusion Matrix (Normalized)')\n",
        "        axes[1].set_xlabel('Predicted')\n",
        "        axes[1].set_ylabel('Actual')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('reports/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        return cm, cm_normalized\n",
        "    \n",
        "    def create_roc_curve_analysis(self):\n",
        "        \"\"\"Create ROC curve analysis for binary classification\"\"\"\n",
        "        if not hasattr(self, 'evaluation_results'):\n",
        "            raise ValueError(\"Run evaluation first!\")\n",
        "        \n",
        "        metrics = self.evaluation_results['metrics']\n",
        "        if metrics['roc_auc'] is None:\n",
        "            print(\"âš ï¸ ROC analysis only available for binary classification\")\n",
        "            return\n",
        "        \n",
        "        y_true = self.evaluation_results['true_labels']\n",
        "        scores = self.evaluation_results['prediction_scores']\n",
        "        \n",
        "        # Calculate ROC curve\n",
        "        fpr, tpr, thresholds = roc_curve(y_true, scores)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        \n",
        "        # Create plot\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
        "                label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig('reports/roc_curve.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        return fpr, tpr, thresholds\n",
        "    \n",
        "    def error_analysis(self, num_examples=10):\n",
        "        \"\"\"Detailed error analysis with examples\"\"\"\n",
        "        if not hasattr(self, 'evaluation_results'):\n",
        "            raise ValueError(\"Run evaluation first!\")\n",
        "        \n",
        "        results = self.evaluation_results\n",
        "        \n",
        "        # Find misclassified examples\n",
        "        misclassified_indices = [\n",
        "            i for i, (true, pred) in enumerate(zip(results['true_labels'], results['predictions']))\n",
        "            if true != pred\n",
        "        ]\n",
        "        \n",
        "        print(f\"Error Analysis: {len(misclassified_indices)} misclassified examples\")\n",
        "        print(f\"Error rate: {len(misclassified_indices)/len(results['true_labels']):.2%}\")\n",
        "        \n",
        "        # Analyze error patterns\n",
        "        error_patterns = {}\n",
        "        for idx in misclassified_indices:\n",
        "            true_label = results['true_labels'][idx]\n",
        "            pred_label = results['predictions'][idx]\n",
        "            pattern = f\"{true_label} -> {pred_label}\"\n",
        "            error_patterns[pattern] = error_patterns.get(pattern, 0) + 1\n",
        "        \n",
        "        print(\"\\nError Patterns:\")\n",
        "        for pattern, count in sorted(error_patterns.items(), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"  {pattern}: {count} examples\")\n",
        "        \n",
        "        # Show examples of misclassified texts\n",
        "        print(f\"\\nSample Misclassified Examples (showing {num_examples}):\")\n",
        "        for i, idx in enumerate(misclassified_indices[:num_examples]):\n",
        "            text = results['texts'][idx]\n",
        "            true_label = results['true_labels'][idx]\n",
        "            pred_label = results['predictions'][idx]\n",
        "            confidence = results['prediction_scores'][idx]\n",
        "            \n",
        "            print(f\"\\nExample {i+1}:\")\n",
        "            print(f\"Text: {text[:200]}...\")\n",
        "            print(f\"True: {true_label}, Predicted: {pred_label}, Confidence: {confidence:.3f}\")\n",
        "        \n",
        "        return misclassified_indices, error_patterns\n",
        "    \n",
        "    def save_evaluation_report(self, output_path='reports/evaluation_metrics.json'):\n",
        "        \"\"\"Save comprehensive evaluation report\"\"\"\n",
        "        if not hasattr(self, 'evaluation_results'):\n",
        "            raise ValueError(\"Run evaluation first!\")\n",
        "        \n",
        "        # Prepare report data\n",
        "        report_data = {\n",
        "            'dataset_info': {\n",
        "                'name': self.evaluation_results['dataset_name'],\n",
        "                'size': len(self.evaluation_results['true_labels'])\n",
        "            },\n",
        "            'metrics': {\n",
        "                'accuracy': float(self.evaluation_results['metrics']['accuracy']),\n",
        "                'precision': float(self.evaluation_results['metrics']['precision']),\n",
        "                'recall': float(self.evaluation_results['metrics']['recall']),\n",
        "                'f1': float(self.evaluation_results['metrics']['f1']),\n",
        "                'roc_auc': float(self.evaluation_results['metrics']['roc_auc']) if self.evaluation_results['metrics']['roc_auc'] else None\n",
        "            },\n",
        "            'per_class_metrics': {\n",
        "                'precision': [float(x) for x in self.evaluation_results['metrics']['per_class_precision']],\n",
        "                'recall': [float(x) for x in self.evaluation_results['metrics']['per_class_recall']],\n",
        "                'f1': [float(x) for x in self.evaluation_results['metrics']['per_class_f1']],\n",
        "                'support': [int(x) for x in self.evaluation_results['metrics']['support']]\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # Save to file\n",
        "        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(report_data, f, indent=2)\n",
        "        \n",
        "        print(f\"Evaluation report saved to {output_path}\")\n",
        "        \n",
        "        return report_data\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = ComprehensiveEvaluator('./models/trained_model', './models/tokenizer')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def comprehensive_evaluation_session():\n",
        "    \"\"\"Interactive evaluation session with multiple analysis options\"\"\"\n",
        "    \n",
        "    # Load test dataset\n",
        "    print(\"Loading test dataset...\")\n",
        "    test_dataset = load_dataset('imdb')['test']\n",
        "    \n",
        "    # Run evaluation\n",
        "    print(\"Running comprehensive evaluation...\")\n",
        "    results = evaluator.evaluate_on_dataset(test_dataset, \"IMDB Test\")\n",
        "    \n",
        "    # Display results\n",
        "    metrics = results['metrics']\n",
        "    print(f\"\\nModel Performance Summary:\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
        "    print(f\"F1 Score:  {metrics['f1']:.4f}\")\n",
        "    if metrics['roc_auc']:\n",
        "        print(f\"ROC AUC:   {metrics['roc_auc']:.4f}\")\n",
        "    \n",
        "    # Interactive analysis menu\n",
        "    while True:\n",
        "        print(f\"\\nAnalysis Options:\")\n",
        "        print(\"1. Confusion Matrix Analysis\")\n",
        "        print(\"2. ROC Curve Analysis\")\n",
        "        print(\"3. Error Analysis\")\n",
        "        print(\"4. Per-Class Performance\")\n",
        "        print(\"5. Save Evaluation Report\")\n",
        "        print(\"6. Generate All Visualizations\")\n",
        "        print(\"0. Exit\")\n",
        "        \n",
        "        choice = input(\"\\nSelect option (0-6): \").strip()\n",
        "        \n",
        "        if choice == '0':\n",
        "            break\n",
        "        elif choice == '1':\n",
        "            evaluator.create_confusion_matrix_plot(['Negative', 'Positive'])\n",
        "        elif choice == '2':\n",
        "            evaluator.create_roc_curve_analysis()\n",
        "        elif choice == '3':\n",
        "            evaluator.error_analysis(num_examples=15)\n",
        "        elif choice == '4':\n",
        "            display_per_class_performance(results)\n",
        "        elif choice == '5':\n",
        "            evaluator.save_evaluation_report()\n",
        "        elif choice == '6':\n",
        "            generate_all_visualizations(results)\n",
        "        else:\n",
        "            print(\"Invalid option!\")\n",
        "\n",
        "def display_per_class_performance(results):\n",
        "    \"\"\"Display detailed per-class performance metrics\"\"\"\n",
        "    metrics = results['metrics']\n",
        "    class_names = ['Negative', 'Positive']  # Adjust based on your dataset\n",
        "    \n",
        "    # Create DataFrame for better display\n",
        "    per_class_df = pd.DataFrame({\n",
        "        'Class': class_names,\n",
        "        'Precision': metrics['per_class_precision'],\n",
        "        'Recall': metrics['per_class_recall'],\n",
        "        'F1-Score': metrics['per_class_f1'],\n",
        "        'Support': metrics['support']\n",
        "    })\n",
        "    \n",
        "    print(\"\\nðŸ“ˆ Per-Class Performance:\")\n",
        "    print(per_class_df.round(4))\n",
        "    \n",
        "    # Visualize per-class metrics\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    \n",
        "    metrics_to_plot = ['Precision', 'Recall', 'F1-Score']\n",
        "    colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
        "    \n",
        "    for i, metric in enumerate(metrics_to_plot):\n",
        "        axes[i].bar(class_names, per_class_df[metric], color=colors[i], alpha=0.7)\n",
        "        axes[i].set_title(f'Per-Class {metric}')\n",
        "        axes[i].set_ylabel(metric)\n",
        "        axes[i].set_ylim(0, 1)\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for j, v in enumerate(per_class_df[metric]):\n",
        "            axes[i].text(j, v + 0.02, f'{v:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('reports/per_class_performance.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def generate_all_visualizations(results):\n",
        "    \"\"\"Generate comprehensive visualization suite\"\"\"\n",
        "    print(\" Generating comprehensive visualization suite...\")\n",
        "    \n",
        "    # 1. Confusion Matrix\n",
        "    evaluator.create_confusion_matrix_plot(['Negative', 'Positive'])\n",
        "    \n",
        "    # 2. ROC Curve\n",
        "    evaluator.create_roc_curve_analysis()\n",
        "    \n",
        "    # 3. Per-class performance\n",
        "    display_per_class_performance(results)\n",
        "    \n",
        "    # 4. Performance summary dashboard\n",
        "    create_performance_dashboard(results)\n",
        "    \n",
        "    print(\"All visualizations generated and saved!\")\n",
        "\n",
        "def create_performance_dashboard(results):\n",
        "    \"\"\"Create comprehensive performance dashboard\"\"\"\n",
        "    metrics = results['metrics']\n",
        "    \n",
        "    # Create dashboard figure\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=('Overall Metrics', 'Score Distribution', \n",
        "                       'Error Analysis', 'Model Confidence'),\n",
        "        specs=[[{\"type\": \"bar\"}, {\"type\": \"histogram\"}],\n",
        "               [{\"type\": \"pie\"}, {\"type\": \"scatter\"}]]\n",
        "    )\n",
        "    \n",
        "    # Overall metrics bar chart\n",
        "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
        "    metric_values = [metrics['accuracy'], metrics['precision'], \n",
        "                    metrics['recall'], metrics['f1']]\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Bar(x=metric_names, y=metric_values, \n",
        "               marker_color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    \n",
        "    # Score distribution histogram\n",
        "    scores = results['prediction_scores']\n",
        "    fig.add_trace(\n",
        "        go.Histogram(x=scores, nbinsx=30, marker_color='lightblue'),\n",
        "        row=1, col=2\n",
        "    )\n",
        "    \n",
        "    # Error analysis pie chart\n",
        "    correct = sum(1 for t, p in zip(results['true_labels'], results['predictions']) if t == p)\n",
        "    incorrect = len(results['true_labels']) - correct\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Pie(labels=['Correct', 'Incorrect'], values=[correct, incorrect],\n",
        "               marker_colors=['lightgreen', 'lightcoral']),\n",
        "        row=2, col=1\n",
        "    )\n",
        "    \n",
        "    # Confidence vs Accuracy scatter\n",
        "    confidences = results['prediction_scores']\n",
        "    accuracies = [1 if t == p else 0 for t, p in zip(results['true_labels'], results['predictions'])]\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=confidences, y=accuracies, mode='markers',\n",
        "                  marker=dict(color=accuracies, colorscale='RdYlBu')),\n",
        "        row=2, col=2\n",
        "    )\n",
        "    \n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        title_text=\"Model Performance Dashboard\",\n",
        "        showlegend=False,\n",
        "        height=800\n",
        "    )\n",
        "    \n",
        "    fig.write_html('reports/performance_dashboard.html')\n",
        "    fig.show()\n",
        "\n",
        "# Run comprehensive evaluation\n",
        "print(\"Starting Comprehensive Evaluation Session...\")\n",
        "comprehensive_evaluation_session()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
